_tmpl_begin(TY = [[fnc;u8; u16; u32; u64; ptr; sz]]);
#ifndef VATOMIC_CONFIG_UPCASE_TY__RLX_H
#define VATOMIC_CONFIG_UPCASE_TY__RLX_H
_tmpl_end;
_tmpl_begin(=);
AUTOGEN
_tmpl_end;

#include <vsync/atomic/await.h>

#define _tmpl_mute
#include <vsync/atomic/tmplr.h>
#define _tmpl_unmute

#if defined(VATOMIC_ENABLE_ATOMIC_RLX)

_tmpl_dl; // fence
_tmpl_begin(TY = [[fnc]], MO = [[seq; acq; rel]]);
#define _tmpl_upcase(vatomic_fence_MS)
static inline void vatomic_fence_MS(void) {
     vatomic_fence_rlx();
}
_tmpl_end;

_tmpl_dl; // read
_tmpl_begin(TY =  [[u8; u16; u32; u64; sz; ptr]], MO = [[seq; acq;]]);
#define _tmpl_upcase(__vatomic_read_MS)
static inline TT __vatomic_read_MS(const AA *a) {
     return __vatomic_read_rlx(a);
}
_tmpl_end;

_tmpl_dl; // write
_tmpl_begin(TY = [[u8; u16; u32; u64; sz; ptr]], MO = [[seq; rel;]]);
#define _tmpl_upcase(__vatomic_write_MS)
static inline void __vatomic_write_MS(AA *a, TT v) {
     __vatomic_write_rlx(a, v);
}
_tmpl_end;

_tmpl_dl; // xchg
_tmpl_begin(TY = [[u8; u16; u32; u64; sz; ptr]], MO = [[seq; acq; rel]]);
#define _tmpl_upcase(__vatomic_xchg_MS)
static inline TT __vatomic_xchg_MS(AA *a, TT v) {
     return __vatomic_xchg_rlx(a, v);
}
_tmpl_end;

_tmpl_dl; // cmpxchg
_tmpl_begin(TY = [[u8; u16; u32; u64; sz; ptr]], MO = [[seq; acq; rel]]);
#define _tmpl_upcase(__vatomic_cmpxchg_MS)
static inline TT __vatomic_cmpxchg_MS(AA *a, TT e, TT v) {
     return __vatomic_cmpxchg_rlx(a, e, v);
}
_tmpl_end;

_tmpl_dl; // get_op/ op_get
_tmpl_begin(TY = [[u8; u16; u32; u64; sz;]], MO = [[seq; acq; rel]],
     FUNC = [[get_max; get_and; get_or; get_xor; get_add; get_sub; max_get; and_get; or_get; xor_get; add_get; sub_get]]);
#define _tmpl_upcase(__vatomic_FUNC_MS)
static inline TT __vatomic_FUNC_MS(AA *a, TT v) {
     return __vatomic_FUNC_rlx(a, v);
}
_tmpl_end;

_tmpl_begin(TY = [[u8; u16; u32; u64; sz;]], MO = [[seq; acq; rel]],
     FUNC = [[get_inc; inc_get; get_dec; dec_get]]);
#define _tmpl_upcase(__vatomic_FUNC_MS)
static inline TT __vatomic_FUNC_MS(AA *a) {
     return __vatomic_FUNC_rlx(a);
}
_tmpl_end;

_tmpl_dl; // op
_tmpl_begin(TY = [[u8; u16; u32; u64; sz;]], MO = [[seq; rel]],
    FUNC = [[max; and; or; xor; add; sub]]);
#define _tmpl_upcase(__vatomic_FUNC_MS)
static inline void __vatomic_FUNC_MS(AA *a, TT v) {
     __vatomic_FUNC_rlx(a, v);
}
_tmpl_end;

_tmpl_begin(TY = [[u8; u16; u32; u64; sz;]], MO = [[seq; rel]],
    FUNC = [[inc; dec]]);
#define _tmpl_upcase(__vatomic_FUNC_MS)
static inline void __vatomic_FUNC_MS(AA *a) {
     __vatomic_FUNC_rlx(a);
}
_tmpl_end;

_tmpl_dl; // await_cond
_tmpl_begin(TY = [[u32; u64; ptr]], MO = [[seq; acq]],
     COND = [[eq; neq; lt; le; gt; ge]],
     $F_ptr_eq = BLK_KEEP,  $F_ptr_neq = BLK_KEEP, $F_ptr = BLK_SKIP);
$F_TY_COND;
#define _tmpl_upcase(__vatomic_await_COND_MS)
static inline TT __vatomic_await_COND_MS(const AA *a, TT v) {
     return __vatomic_await_COND_rlx(a, v);
}
_tmpl_end;

_tmpl_begin(TY = [[u32; u64]], COND = [[le; lt; ge; gt]],
            OP = [[add; sub; set]], MO = [[seq; acq; rel]],
            FUNC = await_COND_OP);
#define _tmpl_upcase(__vatomic_await_COND_OP_MS)
static inline TT
__vatomic_await_COND_OP_MS(AA *a, TT c, TT v)
{
    return __vatomic_await_COND_OP_rlx(a, c, v);
}
_tmpl_end;

_tmpl_begin(TY = [[u32; u64; ptr]], OP = [[add; sub; set]],
            MO = [[seq; acq; rel]], FUNC = await_neq_OP, $F_ptr_set = BLK_KEEP,
            $F_ptr = BLK_SKIP);
$F_TY_OP;
#define _tmpl_upcase(__vatomic_await_neq_OP_MS)
static inline TT
__vatomic_await_neq_OP_MS(AA *a, TT c, TT v)
{
    return __vatomic_await_neq_OP_rlx(a, c, v);
}
_tmpl_end;

_tmpl_begin(TY = [[u32; u64; ptr]], OP = [[add; sub; set]],
            MO = [[seq; acq; rel]], FUNC = await_eq_OP,
            $F_ptr_set = BLK_KEEP, $F_ptr = BLK_SKIP);
$F_TY_OP;
#define _tmpl_upcase(__vatomic_await_eq_OP_MS)
static inline TT
__vatomic_await_eq_OP_MS(AA *a, TT c, TT v)
{
    return __vatomic_await_eq_OP_rlx(a, c, v);
}
_tmpl_end;

#endif
#endif
