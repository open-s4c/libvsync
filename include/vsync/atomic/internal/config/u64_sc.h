/*
 * Copyright (C) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 * SPDX-License-Identifier: MIT
 */

#ifndef VATOMIC_CONFIG_U64_SC_H
#define VATOMIC_CONFIG_U64_SC_H
/* !!!Warning: File generated by tmpl; DO NOT EDIT.!!! */

#include <vsync/atomic/await.h>

#if defined(VATOMIC_ENABLE_ATOMIC_SC)

    #define VATOMIC64_READ_RLX
static inline vuint64_t
vatomic64_read_rlx(const vatomic64_t *a)
{
    return vatomic64_read(a);
}
    #define VATOMIC64_READ_ACQ
static inline vuint64_t
vatomic64_read_acq(const vatomic64_t *a)
{
    return vatomic64_read(a);
}

    #define VATOMIC64_WRITE_RLX
static inline void
vatomic64_write_rlx(vatomic64_t *a, vuint64_t v)
{
    vatomic64_write(a, v);
}
    #define VATOMIC64_WRITE_REL
static inline void
vatomic64_write_rel(vatomic64_t *a, vuint64_t v)
{
    vatomic64_write(a, v);
}

    #define VATOMIC64_XCHG_RLX
static inline vuint64_t
vatomic64_xchg_rlx(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_xchg(a, v);
}
    #define VATOMIC64_XCHG_ACQ
static inline vuint64_t
vatomic64_xchg_acq(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_xchg(a, v);
}
    #define VATOMIC64_XCHG_REL
static inline vuint64_t
vatomic64_xchg_rel(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_xchg(a, v);
}

    #define VATOMIC64_CMPXCHG_RLX
static inline vuint64_t
vatomic64_cmpxchg_rlx(vatomic64_t *a, vuint64_t e, vuint64_t v)
{
    return vatomic64_cmpxchg(a, e, v);
}
    #define VATOMIC64_CMPXCHG_ACQ
static inline vuint64_t
vatomic64_cmpxchg_acq(vatomic64_t *a, vuint64_t e, vuint64_t v)
{
    return vatomic64_cmpxchg(a, e, v);
}
    #define VATOMIC64_CMPXCHG_REL
static inline vuint64_t
vatomic64_cmpxchg_rel(vatomic64_t *a, vuint64_t e, vuint64_t v)
{
    return vatomic64_cmpxchg(a, e, v);
}

    #define VATOMIC64_GET_MAX_RLX
static inline vuint64_t
vatomic64_get_max_rlx(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_max(a, v);
}
    #define VATOMIC64_GET_AND_RLX
static inline vuint64_t
vatomic64_get_and_rlx(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_and(a, v);
}
    #define VATOMIC64_GET_OR_RLX
static inline vuint64_t
vatomic64_get_or_rlx(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_or(a, v);
}
    #define VATOMIC64_GET_XOR_RLX
static inline vuint64_t
vatomic64_get_xor_rlx(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_xor(a, v);
}
    #define VATOMIC64_GET_ADD_RLX
static inline vuint64_t
vatomic64_get_add_rlx(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_add(a, v);
}
    #define VATOMIC64_GET_SUB_RLX
static inline vuint64_t
vatomic64_get_sub_rlx(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_sub(a, v);
}
    #define VATOMIC64_MAX_GET_RLX
static inline vuint64_t
vatomic64_max_get_rlx(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_max_get(a, v);
}
    #define VATOMIC64_AND_GET_RLX
static inline vuint64_t
vatomic64_and_get_rlx(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_and_get(a, v);
}
    #define VATOMIC64_OR_GET_RLX
static inline vuint64_t
vatomic64_or_get_rlx(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_or_get(a, v);
}
    #define VATOMIC64_XOR_GET_RLX
static inline vuint64_t
vatomic64_xor_get_rlx(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_xor_get(a, v);
}
    #define VATOMIC64_ADD_GET_RLX
static inline vuint64_t
vatomic64_add_get_rlx(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_add_get(a, v);
}
    #define VATOMIC64_SUB_GET_RLX
static inline vuint64_t
vatomic64_sub_get_rlx(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_sub_get(a, v);
}
    #define VATOMIC64_GET_MAX_ACQ
static inline vuint64_t
vatomic64_get_max_acq(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_max(a, v);
}
    #define VATOMIC64_GET_AND_ACQ
static inline vuint64_t
vatomic64_get_and_acq(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_and(a, v);
}
    #define VATOMIC64_GET_OR_ACQ
static inline vuint64_t
vatomic64_get_or_acq(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_or(a, v);
}
    #define VATOMIC64_GET_XOR_ACQ
static inline vuint64_t
vatomic64_get_xor_acq(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_xor(a, v);
}
    #define VATOMIC64_GET_ADD_ACQ
static inline vuint64_t
vatomic64_get_add_acq(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_add(a, v);
}
    #define VATOMIC64_GET_SUB_ACQ
static inline vuint64_t
vatomic64_get_sub_acq(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_sub(a, v);
}
    #define VATOMIC64_MAX_GET_ACQ
static inline vuint64_t
vatomic64_max_get_acq(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_max_get(a, v);
}
    #define VATOMIC64_AND_GET_ACQ
static inline vuint64_t
vatomic64_and_get_acq(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_and_get(a, v);
}
    #define VATOMIC64_OR_GET_ACQ
static inline vuint64_t
vatomic64_or_get_acq(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_or_get(a, v);
}
    #define VATOMIC64_XOR_GET_ACQ
static inline vuint64_t
vatomic64_xor_get_acq(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_xor_get(a, v);
}
    #define VATOMIC64_ADD_GET_ACQ
static inline vuint64_t
vatomic64_add_get_acq(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_add_get(a, v);
}
    #define VATOMIC64_SUB_GET_ACQ
static inline vuint64_t
vatomic64_sub_get_acq(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_sub_get(a, v);
}
    #define VATOMIC64_GET_MAX_REL
static inline vuint64_t
vatomic64_get_max_rel(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_max(a, v);
}
    #define VATOMIC64_GET_AND_REL
static inline vuint64_t
vatomic64_get_and_rel(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_and(a, v);
}
    #define VATOMIC64_GET_OR_REL
static inline vuint64_t
vatomic64_get_or_rel(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_or(a, v);
}
    #define VATOMIC64_GET_XOR_REL
static inline vuint64_t
vatomic64_get_xor_rel(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_xor(a, v);
}
    #define VATOMIC64_GET_ADD_REL
static inline vuint64_t
vatomic64_get_add_rel(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_add(a, v);
}
    #define VATOMIC64_GET_SUB_REL
static inline vuint64_t
vatomic64_get_sub_rel(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_sub(a, v);
}
    #define VATOMIC64_MAX_GET_REL
static inline vuint64_t
vatomic64_max_get_rel(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_max_get(a, v);
}
    #define VATOMIC64_AND_GET_REL
static inline vuint64_t
vatomic64_and_get_rel(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_and_get(a, v);
}
    #define VATOMIC64_OR_GET_REL
static inline vuint64_t
vatomic64_or_get_rel(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_or_get(a, v);
}
    #define VATOMIC64_XOR_GET_REL
static inline vuint64_t
vatomic64_xor_get_rel(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_xor_get(a, v);
}
    #define VATOMIC64_ADD_GET_REL
static inline vuint64_t
vatomic64_add_get_rel(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_add_get(a, v);
}
    #define VATOMIC64_SUB_GET_REL
static inline vuint64_t
vatomic64_sub_get_rel(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_sub_get(a, v);
}

    #define VATOMIC64_GET_INC_RLX
static inline vuint64_t
vatomic64_get_inc_rlx(vatomic64_t *a)
{
    return vatomic64_get_inc(a);
}
    #define VATOMIC64_INC_GET_RLX
static inline vuint64_t
vatomic64_inc_get_rlx(vatomic64_t *a)
{
    return vatomic64_inc_get(a);
}
    #define VATOMIC64_GET_DEC_RLX
static inline vuint64_t
vatomic64_get_dec_rlx(vatomic64_t *a)
{
    return vatomic64_get_dec(a);
}
    #define VATOMIC64_DEC_GET_RLX
static inline vuint64_t
vatomic64_dec_get_rlx(vatomic64_t *a)
{
    return vatomic64_dec_get(a);
}
    #define VATOMIC64_GET_INC_ACQ
static inline vuint64_t
vatomic64_get_inc_acq(vatomic64_t *a)
{
    return vatomic64_get_inc(a);
}
    #define VATOMIC64_INC_GET_ACQ
static inline vuint64_t
vatomic64_inc_get_acq(vatomic64_t *a)
{
    return vatomic64_inc_get(a);
}
    #define VATOMIC64_GET_DEC_ACQ
static inline vuint64_t
vatomic64_get_dec_acq(vatomic64_t *a)
{
    return vatomic64_get_dec(a);
}
    #define VATOMIC64_DEC_GET_ACQ
static inline vuint64_t
vatomic64_dec_get_acq(vatomic64_t *a)
{
    return vatomic64_dec_get(a);
}
    #define VATOMIC64_GET_INC_REL
static inline vuint64_t
vatomic64_get_inc_rel(vatomic64_t *a)
{
    return vatomic64_get_inc(a);
}
    #define VATOMIC64_INC_GET_REL
static inline vuint64_t
vatomic64_inc_get_rel(vatomic64_t *a)
{
    return vatomic64_inc_get(a);
}
    #define VATOMIC64_GET_DEC_REL
static inline vuint64_t
vatomic64_get_dec_rel(vatomic64_t *a)
{
    return vatomic64_get_dec(a);
}
    #define VATOMIC64_DEC_GET_REL
static inline vuint64_t
vatomic64_dec_get_rel(vatomic64_t *a)
{
    return vatomic64_dec_get(a);
}

    #define VATOMIC64_MAX_RLX
static inline void
vatomic64_max_rlx(vatomic64_t *a, vuint64_t v)
{
    vatomic64_max(a, v);
}
    #define VATOMIC64_AND_RLX
static inline void
vatomic64_and_rlx(vatomic64_t *a, vuint64_t v)
{
    vatomic64_and(a, v);
}
    #define VATOMIC64_OR_RLX
static inline void
vatomic64_or_rlx(vatomic64_t *a, vuint64_t v)
{
    vatomic64_or(a, v);
}
    #define VATOMIC64_XOR_RLX
static inline void
vatomic64_xor_rlx(vatomic64_t *a, vuint64_t v)
{
    vatomic64_xor(a, v);
}
    #define VATOMIC64_ADD_RLX
static inline void
vatomic64_add_rlx(vatomic64_t *a, vuint64_t v)
{
    vatomic64_add(a, v);
}
    #define VATOMIC64_SUB_RLX
static inline void
vatomic64_sub_rlx(vatomic64_t *a, vuint64_t v)
{
    vatomic64_sub(a, v);
}
    #define VATOMIC64_MAX_REL
static inline void
vatomic64_max_rel(vatomic64_t *a, vuint64_t v)
{
    vatomic64_max(a, v);
}
    #define VATOMIC64_AND_REL
static inline void
vatomic64_and_rel(vatomic64_t *a, vuint64_t v)
{
    vatomic64_and(a, v);
}
    #define VATOMIC64_OR_REL
static inline void
vatomic64_or_rel(vatomic64_t *a, vuint64_t v)
{
    vatomic64_or(a, v);
}
    #define VATOMIC64_XOR_REL
static inline void
vatomic64_xor_rel(vatomic64_t *a, vuint64_t v)
{
    vatomic64_xor(a, v);
}
    #define VATOMIC64_ADD_REL
static inline void
vatomic64_add_rel(vatomic64_t *a, vuint64_t v)
{
    vatomic64_add(a, v);
}
    #define VATOMIC64_SUB_REL
static inline void
vatomic64_sub_rel(vatomic64_t *a, vuint64_t v)
{
    vatomic64_sub(a, v);
}

    #define VATOMIC64_INC_RLX
static inline void
vatomic64_inc_rlx(vatomic64_t *a)
{
    vatomic64_inc(a);
}
    #define VATOMIC64_DEC_RLX
static inline void
vatomic64_dec_rlx(vatomic64_t *a)
{
    vatomic64_dec(a);
}
    #define VATOMIC64_INC_REL
static inline void
vatomic64_inc_rel(vatomic64_t *a)
{
    vatomic64_inc(a);
}
    #define VATOMIC64_DEC_REL
static inline void
vatomic64_dec_rel(vatomic64_t *a)
{
    vatomic64_dec(a);
}

    #define VATOMIC64_AWAIT_EQ_RLX
static inline vuint64_t
vatomic64_await_eq_rlx(const vatomic64_t *a, vuint64_t v)
{
    return vatomic64_await_eq(a, v);
}
    #define VATOMIC64_AWAIT_NEQ_RLX
static inline vuint64_t
vatomic64_await_neq_rlx(const vatomic64_t *a, vuint64_t v)
{
    return vatomic64_await_neq(a, v);
}
    #define VATOMIC64_AWAIT_LT_RLX
static inline vuint64_t
vatomic64_await_lt_rlx(const vatomic64_t *a, vuint64_t v)
{
    return vatomic64_await_lt(a, v);
}
    #define VATOMIC64_AWAIT_LE_RLX
static inline vuint64_t
vatomic64_await_le_rlx(const vatomic64_t *a, vuint64_t v)
{
    return vatomic64_await_le(a, v);
}
    #define VATOMIC64_AWAIT_GT_RLX
static inline vuint64_t
vatomic64_await_gt_rlx(const vatomic64_t *a, vuint64_t v)
{
    return vatomic64_await_gt(a, v);
}
    #define VATOMIC64_AWAIT_GE_RLX
static inline vuint64_t
vatomic64_await_ge_rlx(const vatomic64_t *a, vuint64_t v)
{
    return vatomic64_await_ge(a, v);
}
    #define VATOMIC64_AWAIT_EQ_ACQ
static inline vuint64_t
vatomic64_await_eq_acq(const vatomic64_t *a, vuint64_t v)
{
    return vatomic64_await_eq(a, v);
}
    #define VATOMIC64_AWAIT_NEQ_ACQ
static inline vuint64_t
vatomic64_await_neq_acq(const vatomic64_t *a, vuint64_t v)
{
    return vatomic64_await_neq(a, v);
}
    #define VATOMIC64_AWAIT_LT_ACQ
static inline vuint64_t
vatomic64_await_lt_acq(const vatomic64_t *a, vuint64_t v)
{
    return vatomic64_await_lt(a, v);
}
    #define VATOMIC64_AWAIT_LE_ACQ
static inline vuint64_t
vatomic64_await_le_acq(const vatomic64_t *a, vuint64_t v)
{
    return vatomic64_await_le(a, v);
}
    #define VATOMIC64_AWAIT_GT_ACQ
static inline vuint64_t
vatomic64_await_gt_acq(const vatomic64_t *a, vuint64_t v)
{
    return vatomic64_await_gt(a, v);
}
    #define VATOMIC64_AWAIT_GE_ACQ
static inline vuint64_t
vatomic64_await_ge_acq(const vatomic64_t *a, vuint64_t v)
{
    return vatomic64_await_ge(a, v);
}

    #define VATOMIC64_AWAIT_LE_ADD_RLX
static inline vuint64_t
vatomic64_await_le_add_rlx(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_le_add(a, c, v);
}
    #define VATOMIC64_AWAIT_LE_ADD_ACQ
static inline vuint64_t
vatomic64_await_le_add_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_le_add(a, c, v);
}
    #define VATOMIC64_AWAIT_LE_ADD_REL
static inline vuint64_t
vatomic64_await_le_add_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_le_add(a, c, v);
}
    #define VATOMIC64_AWAIT_LE_SUB_RLX
static inline vuint64_t
vatomic64_await_le_sub_rlx(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_le_sub(a, c, v);
}
    #define VATOMIC64_AWAIT_LE_SUB_ACQ
static inline vuint64_t
vatomic64_await_le_sub_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_le_sub(a, c, v);
}
    #define VATOMIC64_AWAIT_LE_SUB_REL
static inline vuint64_t
vatomic64_await_le_sub_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_le_sub(a, c, v);
}
    #define VATOMIC64_AWAIT_LE_SET_RLX
static inline vuint64_t
vatomic64_await_le_set_rlx(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_le_set(a, c, v);
}
    #define VATOMIC64_AWAIT_LE_SET_ACQ
static inline vuint64_t
vatomic64_await_le_set_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_le_set(a, c, v);
}
    #define VATOMIC64_AWAIT_LE_SET_REL
static inline vuint64_t
vatomic64_await_le_set_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_le_set(a, c, v);
}
    #define VATOMIC64_AWAIT_LT_ADD_RLX
static inline vuint64_t
vatomic64_await_lt_add_rlx(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_lt_add(a, c, v);
}
    #define VATOMIC64_AWAIT_LT_ADD_ACQ
static inline vuint64_t
vatomic64_await_lt_add_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_lt_add(a, c, v);
}
    #define VATOMIC64_AWAIT_LT_ADD_REL
static inline vuint64_t
vatomic64_await_lt_add_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_lt_add(a, c, v);
}
    #define VATOMIC64_AWAIT_LT_SUB_RLX
static inline vuint64_t
vatomic64_await_lt_sub_rlx(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_lt_sub(a, c, v);
}
    #define VATOMIC64_AWAIT_LT_SUB_ACQ
static inline vuint64_t
vatomic64_await_lt_sub_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_lt_sub(a, c, v);
}
    #define VATOMIC64_AWAIT_LT_SUB_REL
static inline vuint64_t
vatomic64_await_lt_sub_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_lt_sub(a, c, v);
}
    #define VATOMIC64_AWAIT_LT_SET_RLX
static inline vuint64_t
vatomic64_await_lt_set_rlx(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_lt_set(a, c, v);
}
    #define VATOMIC64_AWAIT_LT_SET_ACQ
static inline vuint64_t
vatomic64_await_lt_set_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_lt_set(a, c, v);
}
    #define VATOMIC64_AWAIT_LT_SET_REL
static inline vuint64_t
vatomic64_await_lt_set_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_lt_set(a, c, v);
}
    #define VATOMIC64_AWAIT_GE_ADD_RLX
static inline vuint64_t
vatomic64_await_ge_add_rlx(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_ge_add(a, c, v);
}
    #define VATOMIC64_AWAIT_GE_ADD_ACQ
static inline vuint64_t
vatomic64_await_ge_add_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_ge_add(a, c, v);
}
    #define VATOMIC64_AWAIT_GE_ADD_REL
static inline vuint64_t
vatomic64_await_ge_add_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_ge_add(a, c, v);
}
    #define VATOMIC64_AWAIT_GE_SUB_RLX
static inline vuint64_t
vatomic64_await_ge_sub_rlx(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_ge_sub(a, c, v);
}
    #define VATOMIC64_AWAIT_GE_SUB_ACQ
static inline vuint64_t
vatomic64_await_ge_sub_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_ge_sub(a, c, v);
}
    #define VATOMIC64_AWAIT_GE_SUB_REL
static inline vuint64_t
vatomic64_await_ge_sub_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_ge_sub(a, c, v);
}
    #define VATOMIC64_AWAIT_GE_SET_RLX
static inline vuint64_t
vatomic64_await_ge_set_rlx(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_ge_set(a, c, v);
}
    #define VATOMIC64_AWAIT_GE_SET_ACQ
static inline vuint64_t
vatomic64_await_ge_set_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_ge_set(a, c, v);
}
    #define VATOMIC64_AWAIT_GE_SET_REL
static inline vuint64_t
vatomic64_await_ge_set_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_ge_set(a, c, v);
}
    #define VATOMIC64_AWAIT_GT_ADD_RLX
static inline vuint64_t
vatomic64_await_gt_add_rlx(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_gt_add(a, c, v);
}
    #define VATOMIC64_AWAIT_GT_ADD_ACQ
static inline vuint64_t
vatomic64_await_gt_add_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_gt_add(a, c, v);
}
    #define VATOMIC64_AWAIT_GT_ADD_REL
static inline vuint64_t
vatomic64_await_gt_add_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_gt_add(a, c, v);
}
    #define VATOMIC64_AWAIT_GT_SUB_RLX
static inline vuint64_t
vatomic64_await_gt_sub_rlx(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_gt_sub(a, c, v);
}
    #define VATOMIC64_AWAIT_GT_SUB_ACQ
static inline vuint64_t
vatomic64_await_gt_sub_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_gt_sub(a, c, v);
}
    #define VATOMIC64_AWAIT_GT_SUB_REL
static inline vuint64_t
vatomic64_await_gt_sub_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_gt_sub(a, c, v);
}
    #define VATOMIC64_AWAIT_GT_SET_RLX
static inline vuint64_t
vatomic64_await_gt_set_rlx(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_gt_set(a, c, v);
}
    #define VATOMIC64_AWAIT_GT_SET_ACQ
static inline vuint64_t
vatomic64_await_gt_set_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_gt_set(a, c, v);
}
    #define VATOMIC64_AWAIT_GT_SET_REL
static inline vuint64_t
vatomic64_await_gt_set_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_gt_set(a, c, v);
}

    #define VATOMIC64_AWAIT_NEQ_ADD_RLX
static inline vuint64_t
vatomic64_await_neq_add_rlx(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_neq_add(a, c, v);
}
    #define VATOMIC64_AWAIT_NEQ_ADD_ACQ
static inline vuint64_t
vatomic64_await_neq_add_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_neq_add(a, c, v);
}
    #define VATOMIC64_AWAIT_NEQ_ADD_REL
static inline vuint64_t
vatomic64_await_neq_add_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_neq_add(a, c, v);
}
    #define VATOMIC64_AWAIT_NEQ_SUB_RLX
static inline vuint64_t
vatomic64_await_neq_sub_rlx(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_neq_sub(a, c, v);
}
    #define VATOMIC64_AWAIT_NEQ_SUB_ACQ
static inline vuint64_t
vatomic64_await_neq_sub_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_neq_sub(a, c, v);
}
    #define VATOMIC64_AWAIT_NEQ_SUB_REL
static inline vuint64_t
vatomic64_await_neq_sub_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_neq_sub(a, c, v);
}
    #define VATOMIC64_AWAIT_NEQ_SET_RLX
static inline vuint64_t
vatomic64_await_neq_set_rlx(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_neq_set(a, c, v);
}
    #define VATOMIC64_AWAIT_NEQ_SET_ACQ
static inline vuint64_t
vatomic64_await_neq_set_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_neq_set(a, c, v);
}
    #define VATOMIC64_AWAIT_NEQ_SET_REL
static inline vuint64_t
vatomic64_await_neq_set_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_neq_set(a, c, v);
}

    #define VATOMIC64_AWAIT_EQ_ADD_RLX
static inline vuint64_t
vatomic64_await_eq_add_rlx(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_eq_add(a, c, v);
}
    #define VATOMIC64_AWAIT_EQ_ADD_ACQ
static inline vuint64_t
vatomic64_await_eq_add_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_eq_add(a, c, v);
}
    #define VATOMIC64_AWAIT_EQ_ADD_REL
static inline vuint64_t
vatomic64_await_eq_add_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_eq_add(a, c, v);
}
    #define VATOMIC64_AWAIT_EQ_SUB_RLX
static inline vuint64_t
vatomic64_await_eq_sub_rlx(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_eq_sub(a, c, v);
}
    #define VATOMIC64_AWAIT_EQ_SUB_ACQ
static inline vuint64_t
vatomic64_await_eq_sub_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_eq_sub(a, c, v);
}
    #define VATOMIC64_AWAIT_EQ_SUB_REL
static inline vuint64_t
vatomic64_await_eq_sub_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_eq_sub(a, c, v);
}
    #define VATOMIC64_AWAIT_EQ_SET_RLX
static inline vuint64_t
vatomic64_await_eq_set_rlx(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_eq_set(a, c, v);
}
    #define VATOMIC64_AWAIT_EQ_SET_ACQ
static inline vuint64_t
vatomic64_await_eq_set_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_eq_set(a, c, v);
}
    #define VATOMIC64_AWAIT_EQ_SET_REL
static inline vuint64_t
vatomic64_await_eq_set_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_eq_set(a, c, v);
}

#endif
#endif
