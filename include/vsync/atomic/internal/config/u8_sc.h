/*
 * Copyright (C) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 * SPDX-License-Identifier: MIT
 */

#ifndef VATOMIC_CONFIG_U8_SC_H
#define VATOMIC_CONFIG_U8_SC_H
/* !!!Warning: File generated by tmpl; DO NOT EDIT.!!! */

#include <vsync/atomic/await.h>

#if defined(VATOMIC_ENABLE_ATOMIC_SC)

    #define VATOMIC8_READ_RLX
static inline vuint8_t
vatomic8_read_rlx(const vatomic8_t *a)
{
    return vatomic8_read(a);
}
    #define VATOMIC8_READ_ACQ
static inline vuint8_t
vatomic8_read_acq(const vatomic8_t *a)
{
    return vatomic8_read(a);
}

    #define VATOMIC8_WRITE_RLX
static inline void
vatomic8_write_rlx(vatomic8_t *a, vuint8_t v)
{
    vatomic8_write(a, v);
}
    #define VATOMIC8_WRITE_REL
static inline void
vatomic8_write_rel(vatomic8_t *a, vuint8_t v)
{
    vatomic8_write(a, v);
}

    #define VATOMIC8_XCHG_RLX
static inline vuint8_t
vatomic8_xchg_rlx(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_xchg(a, v);
}
    #define VATOMIC8_XCHG_ACQ
static inline vuint8_t
vatomic8_xchg_acq(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_xchg(a, v);
}
    #define VATOMIC8_XCHG_REL
static inline vuint8_t
vatomic8_xchg_rel(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_xchg(a, v);
}

    #define VATOMIC8_CMPXCHG_RLX
static inline vuint8_t
vatomic8_cmpxchg_rlx(vatomic8_t *a, vuint8_t e, vuint8_t v)
{
    return vatomic8_cmpxchg(a, e, v);
}
    #define VATOMIC8_CMPXCHG_ACQ
static inline vuint8_t
vatomic8_cmpxchg_acq(vatomic8_t *a, vuint8_t e, vuint8_t v)
{
    return vatomic8_cmpxchg(a, e, v);
}
    #define VATOMIC8_CMPXCHG_REL
static inline vuint8_t
vatomic8_cmpxchg_rel(vatomic8_t *a, vuint8_t e, vuint8_t v)
{
    return vatomic8_cmpxchg(a, e, v);
}

    #define VATOMIC8_GET_MAX_RLX
static inline vuint8_t
vatomic8_get_max_rlx(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_get_max(a, v);
}
    #define VATOMIC8_GET_AND_RLX
static inline vuint8_t
vatomic8_get_and_rlx(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_get_and(a, v);
}
    #define VATOMIC8_GET_OR_RLX
static inline vuint8_t
vatomic8_get_or_rlx(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_get_or(a, v);
}
    #define VATOMIC8_GET_XOR_RLX
static inline vuint8_t
vatomic8_get_xor_rlx(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_get_xor(a, v);
}
    #define VATOMIC8_GET_ADD_RLX
static inline vuint8_t
vatomic8_get_add_rlx(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_get_add(a, v);
}
    #define VATOMIC8_GET_SUB_RLX
static inline vuint8_t
vatomic8_get_sub_rlx(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_get_sub(a, v);
}
    #define VATOMIC8_MAX_GET_RLX
static inline vuint8_t
vatomic8_max_get_rlx(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_max_get(a, v);
}
    #define VATOMIC8_AND_GET_RLX
static inline vuint8_t
vatomic8_and_get_rlx(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_and_get(a, v);
}
    #define VATOMIC8_OR_GET_RLX
static inline vuint8_t
vatomic8_or_get_rlx(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_or_get(a, v);
}
    #define VATOMIC8_XOR_GET_RLX
static inline vuint8_t
vatomic8_xor_get_rlx(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_xor_get(a, v);
}
    #define VATOMIC8_ADD_GET_RLX
static inline vuint8_t
vatomic8_add_get_rlx(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_add_get(a, v);
}
    #define VATOMIC8_SUB_GET_RLX
static inline vuint8_t
vatomic8_sub_get_rlx(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_sub_get(a, v);
}
    #define VATOMIC8_GET_MAX_ACQ
static inline vuint8_t
vatomic8_get_max_acq(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_get_max(a, v);
}
    #define VATOMIC8_GET_AND_ACQ
static inline vuint8_t
vatomic8_get_and_acq(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_get_and(a, v);
}
    #define VATOMIC8_GET_OR_ACQ
static inline vuint8_t
vatomic8_get_or_acq(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_get_or(a, v);
}
    #define VATOMIC8_GET_XOR_ACQ
static inline vuint8_t
vatomic8_get_xor_acq(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_get_xor(a, v);
}
    #define VATOMIC8_GET_ADD_ACQ
static inline vuint8_t
vatomic8_get_add_acq(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_get_add(a, v);
}
    #define VATOMIC8_GET_SUB_ACQ
static inline vuint8_t
vatomic8_get_sub_acq(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_get_sub(a, v);
}
    #define VATOMIC8_MAX_GET_ACQ
static inline vuint8_t
vatomic8_max_get_acq(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_max_get(a, v);
}
    #define VATOMIC8_AND_GET_ACQ
static inline vuint8_t
vatomic8_and_get_acq(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_and_get(a, v);
}
    #define VATOMIC8_OR_GET_ACQ
static inline vuint8_t
vatomic8_or_get_acq(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_or_get(a, v);
}
    #define VATOMIC8_XOR_GET_ACQ
static inline vuint8_t
vatomic8_xor_get_acq(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_xor_get(a, v);
}
    #define VATOMIC8_ADD_GET_ACQ
static inline vuint8_t
vatomic8_add_get_acq(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_add_get(a, v);
}
    #define VATOMIC8_SUB_GET_ACQ
static inline vuint8_t
vatomic8_sub_get_acq(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_sub_get(a, v);
}
    #define VATOMIC8_GET_MAX_REL
static inline vuint8_t
vatomic8_get_max_rel(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_get_max(a, v);
}
    #define VATOMIC8_GET_AND_REL
static inline vuint8_t
vatomic8_get_and_rel(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_get_and(a, v);
}
    #define VATOMIC8_GET_OR_REL
static inline vuint8_t
vatomic8_get_or_rel(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_get_or(a, v);
}
    #define VATOMIC8_GET_XOR_REL
static inline vuint8_t
vatomic8_get_xor_rel(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_get_xor(a, v);
}
    #define VATOMIC8_GET_ADD_REL
static inline vuint8_t
vatomic8_get_add_rel(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_get_add(a, v);
}
    #define VATOMIC8_GET_SUB_REL
static inline vuint8_t
vatomic8_get_sub_rel(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_get_sub(a, v);
}
    #define VATOMIC8_MAX_GET_REL
static inline vuint8_t
vatomic8_max_get_rel(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_max_get(a, v);
}
    #define VATOMIC8_AND_GET_REL
static inline vuint8_t
vatomic8_and_get_rel(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_and_get(a, v);
}
    #define VATOMIC8_OR_GET_REL
static inline vuint8_t
vatomic8_or_get_rel(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_or_get(a, v);
}
    #define VATOMIC8_XOR_GET_REL
static inline vuint8_t
vatomic8_xor_get_rel(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_xor_get(a, v);
}
    #define VATOMIC8_ADD_GET_REL
static inline vuint8_t
vatomic8_add_get_rel(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_add_get(a, v);
}
    #define VATOMIC8_SUB_GET_REL
static inline vuint8_t
vatomic8_sub_get_rel(vatomic8_t *a, vuint8_t v)
{
    return vatomic8_sub_get(a, v);
}

    #define VATOMIC8_GET_INC_RLX
static inline vuint8_t
vatomic8_get_inc_rlx(vatomic8_t *a)
{
    return vatomic8_get_inc(a);
}
    #define VATOMIC8_INC_GET_RLX
static inline vuint8_t
vatomic8_inc_get_rlx(vatomic8_t *a)
{
    return vatomic8_inc_get(a);
}
    #define VATOMIC8_GET_DEC_RLX
static inline vuint8_t
vatomic8_get_dec_rlx(vatomic8_t *a)
{
    return vatomic8_get_dec(a);
}
    #define VATOMIC8_DEC_GET_RLX
static inline vuint8_t
vatomic8_dec_get_rlx(vatomic8_t *a)
{
    return vatomic8_dec_get(a);
}
    #define VATOMIC8_GET_INC_ACQ
static inline vuint8_t
vatomic8_get_inc_acq(vatomic8_t *a)
{
    return vatomic8_get_inc(a);
}
    #define VATOMIC8_INC_GET_ACQ
static inline vuint8_t
vatomic8_inc_get_acq(vatomic8_t *a)
{
    return vatomic8_inc_get(a);
}
    #define VATOMIC8_GET_DEC_ACQ
static inline vuint8_t
vatomic8_get_dec_acq(vatomic8_t *a)
{
    return vatomic8_get_dec(a);
}
    #define VATOMIC8_DEC_GET_ACQ
static inline vuint8_t
vatomic8_dec_get_acq(vatomic8_t *a)
{
    return vatomic8_dec_get(a);
}
    #define VATOMIC8_GET_INC_REL
static inline vuint8_t
vatomic8_get_inc_rel(vatomic8_t *a)
{
    return vatomic8_get_inc(a);
}
    #define VATOMIC8_INC_GET_REL
static inline vuint8_t
vatomic8_inc_get_rel(vatomic8_t *a)
{
    return vatomic8_inc_get(a);
}
    #define VATOMIC8_GET_DEC_REL
static inline vuint8_t
vatomic8_get_dec_rel(vatomic8_t *a)
{
    return vatomic8_get_dec(a);
}
    #define VATOMIC8_DEC_GET_REL
static inline vuint8_t
vatomic8_dec_get_rel(vatomic8_t *a)
{
    return vatomic8_dec_get(a);
}

    #define VATOMIC8_MAX_RLX
static inline void
vatomic8_max_rlx(vatomic8_t *a, vuint8_t v)
{
    vatomic8_max(a, v);
}
    #define VATOMIC8_AND_RLX
static inline void
vatomic8_and_rlx(vatomic8_t *a, vuint8_t v)
{
    vatomic8_and(a, v);
}
    #define VATOMIC8_OR_RLX
static inline void
vatomic8_or_rlx(vatomic8_t *a, vuint8_t v)
{
    vatomic8_or(a, v);
}
    #define VATOMIC8_XOR_RLX
static inline void
vatomic8_xor_rlx(vatomic8_t *a, vuint8_t v)
{
    vatomic8_xor(a, v);
}
    #define VATOMIC8_ADD_RLX
static inline void
vatomic8_add_rlx(vatomic8_t *a, vuint8_t v)
{
    vatomic8_add(a, v);
}
    #define VATOMIC8_SUB_RLX
static inline void
vatomic8_sub_rlx(vatomic8_t *a, vuint8_t v)
{
    vatomic8_sub(a, v);
}
    #define VATOMIC8_MAX_REL
static inline void
vatomic8_max_rel(vatomic8_t *a, vuint8_t v)
{
    vatomic8_max(a, v);
}
    #define VATOMIC8_AND_REL
static inline void
vatomic8_and_rel(vatomic8_t *a, vuint8_t v)
{
    vatomic8_and(a, v);
}
    #define VATOMIC8_OR_REL
static inline void
vatomic8_or_rel(vatomic8_t *a, vuint8_t v)
{
    vatomic8_or(a, v);
}
    #define VATOMIC8_XOR_REL
static inline void
vatomic8_xor_rel(vatomic8_t *a, vuint8_t v)
{
    vatomic8_xor(a, v);
}
    #define VATOMIC8_ADD_REL
static inline void
vatomic8_add_rel(vatomic8_t *a, vuint8_t v)
{
    vatomic8_add(a, v);
}
    #define VATOMIC8_SUB_REL
static inline void
vatomic8_sub_rel(vatomic8_t *a, vuint8_t v)
{
    vatomic8_sub(a, v);
}

    #define VATOMIC8_INC_RLX
static inline void
vatomic8_inc_rlx(vatomic8_t *a)
{
    vatomic8_inc(a);
}
    #define VATOMIC8_DEC_RLX
static inline void
vatomic8_dec_rlx(vatomic8_t *a)
{
    vatomic8_dec(a);
}
    #define VATOMIC8_INC_REL
static inline void
vatomic8_inc_rel(vatomic8_t *a)
{
    vatomic8_inc(a);
}
    #define VATOMIC8_DEC_REL
static inline void
vatomic8_dec_rel(vatomic8_t *a)
{
    vatomic8_dec(a);
}

#endif
#endif
