/*
 * Copyright (C) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 * SPDX-License-Identifier: MIT
 */

#ifndef VATOMIC_CONFIG_U64_RLX_H
#define VATOMIC_CONFIG_U64_RLX_H
/* !!!Warning: File generated by tmpl; DO NOT EDIT.!!! */

#include <vsync/atomic/await.h>

#if defined(VATOMIC_ENABLE_ATOMIC_RLX)

    #define VATOMIC64_READ
static inline vuint64_t
vatomic64_read(const vatomic64_t *a)
{
    return vatomic64_read_rlx(a);
}
    #define VATOMIC64_READ_ACQ
static inline vuint64_t
vatomic64_read_acq(const vatomic64_t *a)
{
    return vatomic64_read_rlx(a);
}

    #define VATOMIC64_WRITE
static inline void
vatomic64_write(vatomic64_t *a, vuint64_t v)
{
    vatomic64_write_rlx(a, v);
}
    #define VATOMIC64_WRITE_REL
static inline void
vatomic64_write_rel(vatomic64_t *a, vuint64_t v)
{
    vatomic64_write_rlx(a, v);
}

    #define VATOMIC64_XCHG
static inline vuint64_t
vatomic64_xchg(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_xchg_rlx(a, v);
}
    #define VATOMIC64_XCHG_ACQ
static inline vuint64_t
vatomic64_xchg_acq(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_xchg_rlx(a, v);
}
    #define VATOMIC64_XCHG_REL
static inline vuint64_t
vatomic64_xchg_rel(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_xchg_rlx(a, v);
}

    #define VATOMIC64_CMPXCHG
static inline vuint64_t
vatomic64_cmpxchg(vatomic64_t *a, vuint64_t e, vuint64_t v)
{
    return vatomic64_cmpxchg_rlx(a, e, v);
}
    #define VATOMIC64_CMPXCHG_ACQ
static inline vuint64_t
vatomic64_cmpxchg_acq(vatomic64_t *a, vuint64_t e, vuint64_t v)
{
    return vatomic64_cmpxchg_rlx(a, e, v);
}
    #define VATOMIC64_CMPXCHG_REL
static inline vuint64_t
vatomic64_cmpxchg_rel(vatomic64_t *a, vuint64_t e, vuint64_t v)
{
    return vatomic64_cmpxchg_rlx(a, e, v);
}

    #define VATOMIC64_GET_MAX
static inline vuint64_t
vatomic64_get_max(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_max_rlx(a, v);
}
    #define VATOMIC64_GET_AND
static inline vuint64_t
vatomic64_get_and(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_and_rlx(a, v);
}
    #define VATOMIC64_GET_OR
static inline vuint64_t
vatomic64_get_or(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_or_rlx(a, v);
}
    #define VATOMIC64_GET_XOR
static inline vuint64_t
vatomic64_get_xor(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_xor_rlx(a, v);
}
    #define VATOMIC64_GET_ADD
static inline vuint64_t
vatomic64_get_add(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_add_rlx(a, v);
}
    #define VATOMIC64_GET_SUB
static inline vuint64_t
vatomic64_get_sub(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_sub_rlx(a, v);
}
    #define VATOMIC64_MAX_GET
static inline vuint64_t
vatomic64_max_get(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_max_get_rlx(a, v);
}
    #define VATOMIC64_AND_GET
static inline vuint64_t
vatomic64_and_get(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_and_get_rlx(a, v);
}
    #define VATOMIC64_OR_GET
static inline vuint64_t
vatomic64_or_get(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_or_get_rlx(a, v);
}
    #define VATOMIC64_XOR_GET
static inline vuint64_t
vatomic64_xor_get(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_xor_get_rlx(a, v);
}
    #define VATOMIC64_ADD_GET
static inline vuint64_t
vatomic64_add_get(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_add_get_rlx(a, v);
}
    #define VATOMIC64_SUB_GET
static inline vuint64_t
vatomic64_sub_get(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_sub_get_rlx(a, v);
}
    #define VATOMIC64_GET_MAX_ACQ
static inline vuint64_t
vatomic64_get_max_acq(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_max_rlx(a, v);
}
    #define VATOMIC64_GET_AND_ACQ
static inline vuint64_t
vatomic64_get_and_acq(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_and_rlx(a, v);
}
    #define VATOMIC64_GET_OR_ACQ
static inline vuint64_t
vatomic64_get_or_acq(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_or_rlx(a, v);
}
    #define VATOMIC64_GET_XOR_ACQ
static inline vuint64_t
vatomic64_get_xor_acq(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_xor_rlx(a, v);
}
    #define VATOMIC64_GET_ADD_ACQ
static inline vuint64_t
vatomic64_get_add_acq(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_add_rlx(a, v);
}
    #define VATOMIC64_GET_SUB_ACQ
static inline vuint64_t
vatomic64_get_sub_acq(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_sub_rlx(a, v);
}
    #define VATOMIC64_MAX_GET_ACQ
static inline vuint64_t
vatomic64_max_get_acq(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_max_get_rlx(a, v);
}
    #define VATOMIC64_AND_GET_ACQ
static inline vuint64_t
vatomic64_and_get_acq(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_and_get_rlx(a, v);
}
    #define VATOMIC64_OR_GET_ACQ
static inline vuint64_t
vatomic64_or_get_acq(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_or_get_rlx(a, v);
}
    #define VATOMIC64_XOR_GET_ACQ
static inline vuint64_t
vatomic64_xor_get_acq(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_xor_get_rlx(a, v);
}
    #define VATOMIC64_ADD_GET_ACQ
static inline vuint64_t
vatomic64_add_get_acq(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_add_get_rlx(a, v);
}
    #define VATOMIC64_SUB_GET_ACQ
static inline vuint64_t
vatomic64_sub_get_acq(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_sub_get_rlx(a, v);
}
    #define VATOMIC64_GET_MAX_REL
static inline vuint64_t
vatomic64_get_max_rel(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_max_rlx(a, v);
}
    #define VATOMIC64_GET_AND_REL
static inline vuint64_t
vatomic64_get_and_rel(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_and_rlx(a, v);
}
    #define VATOMIC64_GET_OR_REL
static inline vuint64_t
vatomic64_get_or_rel(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_or_rlx(a, v);
}
    #define VATOMIC64_GET_XOR_REL
static inline vuint64_t
vatomic64_get_xor_rel(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_xor_rlx(a, v);
}
    #define VATOMIC64_GET_ADD_REL
static inline vuint64_t
vatomic64_get_add_rel(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_add_rlx(a, v);
}
    #define VATOMIC64_GET_SUB_REL
static inline vuint64_t
vatomic64_get_sub_rel(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_get_sub_rlx(a, v);
}
    #define VATOMIC64_MAX_GET_REL
static inline vuint64_t
vatomic64_max_get_rel(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_max_get_rlx(a, v);
}
    #define VATOMIC64_AND_GET_REL
static inline vuint64_t
vatomic64_and_get_rel(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_and_get_rlx(a, v);
}
    #define VATOMIC64_OR_GET_REL
static inline vuint64_t
vatomic64_or_get_rel(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_or_get_rlx(a, v);
}
    #define VATOMIC64_XOR_GET_REL
static inline vuint64_t
vatomic64_xor_get_rel(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_xor_get_rlx(a, v);
}
    #define VATOMIC64_ADD_GET_REL
static inline vuint64_t
vatomic64_add_get_rel(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_add_get_rlx(a, v);
}
    #define VATOMIC64_SUB_GET_REL
static inline vuint64_t
vatomic64_sub_get_rel(vatomic64_t *a, vuint64_t v)
{
    return vatomic64_sub_get_rlx(a, v);
}

    #define VATOMIC64_GET_INC
static inline vuint64_t
vatomic64_get_inc(vatomic64_t *a)
{
    return vatomic64_get_inc_rlx(a);
}
    #define VATOMIC64_INC_GET
static inline vuint64_t
vatomic64_inc_get(vatomic64_t *a)
{
    return vatomic64_inc_get_rlx(a);
}
    #define VATOMIC64_GET_DEC
static inline vuint64_t
vatomic64_get_dec(vatomic64_t *a)
{
    return vatomic64_get_dec_rlx(a);
}
    #define VATOMIC64_DEC_GET
static inline vuint64_t
vatomic64_dec_get(vatomic64_t *a)
{
    return vatomic64_dec_get_rlx(a);
}
    #define VATOMIC64_GET_INC_ACQ
static inline vuint64_t
vatomic64_get_inc_acq(vatomic64_t *a)
{
    return vatomic64_get_inc_rlx(a);
}
    #define VATOMIC64_INC_GET_ACQ
static inline vuint64_t
vatomic64_inc_get_acq(vatomic64_t *a)
{
    return vatomic64_inc_get_rlx(a);
}
    #define VATOMIC64_GET_DEC_ACQ
static inline vuint64_t
vatomic64_get_dec_acq(vatomic64_t *a)
{
    return vatomic64_get_dec_rlx(a);
}
    #define VATOMIC64_DEC_GET_ACQ
static inline vuint64_t
vatomic64_dec_get_acq(vatomic64_t *a)
{
    return vatomic64_dec_get_rlx(a);
}
    #define VATOMIC64_GET_INC_REL
static inline vuint64_t
vatomic64_get_inc_rel(vatomic64_t *a)
{
    return vatomic64_get_inc_rlx(a);
}
    #define VATOMIC64_INC_GET_REL
static inline vuint64_t
vatomic64_inc_get_rel(vatomic64_t *a)
{
    return vatomic64_inc_get_rlx(a);
}
    #define VATOMIC64_GET_DEC_REL
static inline vuint64_t
vatomic64_get_dec_rel(vatomic64_t *a)
{
    return vatomic64_get_dec_rlx(a);
}
    #define VATOMIC64_DEC_GET_REL
static inline vuint64_t
vatomic64_dec_get_rel(vatomic64_t *a)
{
    return vatomic64_dec_get_rlx(a);
}

    #define VATOMIC64_MAX
static inline void
vatomic64_max(vatomic64_t *a, vuint64_t v)
{
    vatomic64_max_rlx(a, v);
}
    #define VATOMIC64_AND
static inline void
vatomic64_and(vatomic64_t *a, vuint64_t v)
{
    vatomic64_and_rlx(a, v);
}
    #define VATOMIC64_OR
static inline void
vatomic64_or(vatomic64_t *a, vuint64_t v)
{
    vatomic64_or_rlx(a, v);
}
    #define VATOMIC64_XOR
static inline void
vatomic64_xor(vatomic64_t *a, vuint64_t v)
{
    vatomic64_xor_rlx(a, v);
}
    #define VATOMIC64_ADD
static inline void
vatomic64_add(vatomic64_t *a, vuint64_t v)
{
    vatomic64_add_rlx(a, v);
}
    #define VATOMIC64_SUB
static inline void
vatomic64_sub(vatomic64_t *a, vuint64_t v)
{
    vatomic64_sub_rlx(a, v);
}
    #define VATOMIC64_MAX_REL
static inline void
vatomic64_max_rel(vatomic64_t *a, vuint64_t v)
{
    vatomic64_max_rlx(a, v);
}
    #define VATOMIC64_AND_REL
static inline void
vatomic64_and_rel(vatomic64_t *a, vuint64_t v)
{
    vatomic64_and_rlx(a, v);
}
    #define VATOMIC64_OR_REL
static inline void
vatomic64_or_rel(vatomic64_t *a, vuint64_t v)
{
    vatomic64_or_rlx(a, v);
}
    #define VATOMIC64_XOR_REL
static inline void
vatomic64_xor_rel(vatomic64_t *a, vuint64_t v)
{
    vatomic64_xor_rlx(a, v);
}
    #define VATOMIC64_ADD_REL
static inline void
vatomic64_add_rel(vatomic64_t *a, vuint64_t v)
{
    vatomic64_add_rlx(a, v);
}
    #define VATOMIC64_SUB_REL
static inline void
vatomic64_sub_rel(vatomic64_t *a, vuint64_t v)
{
    vatomic64_sub_rlx(a, v);
}

    #define VATOMIC64_INC
static inline void
vatomic64_inc(vatomic64_t *a)
{
    vatomic64_inc_rlx(a);
}
    #define VATOMIC64_DEC
static inline void
vatomic64_dec(vatomic64_t *a)
{
    vatomic64_dec_rlx(a);
}
    #define VATOMIC64_INC_REL
static inline void
vatomic64_inc_rel(vatomic64_t *a)
{
    vatomic64_inc_rlx(a);
}
    #define VATOMIC64_DEC_REL
static inline void
vatomic64_dec_rel(vatomic64_t *a)
{
    vatomic64_dec_rlx(a);
}

    #define VATOMIC64_AWAIT_EQ
static inline vuint64_t
vatomic64_await_eq(const vatomic64_t *a, vuint64_t v)
{
    return vatomic64_await_eq_rlx(a, v);
}
    #define VATOMIC64_AWAIT_NEQ
static inline vuint64_t
vatomic64_await_neq(const vatomic64_t *a, vuint64_t v)
{
    return vatomic64_await_neq_rlx(a, v);
}
    #define VATOMIC64_AWAIT_LT
static inline vuint64_t
vatomic64_await_lt(const vatomic64_t *a, vuint64_t v)
{
    return vatomic64_await_lt_rlx(a, v);
}
    #define VATOMIC64_AWAIT_LE
static inline vuint64_t
vatomic64_await_le(const vatomic64_t *a, vuint64_t v)
{
    return vatomic64_await_le_rlx(a, v);
}
    #define VATOMIC64_AWAIT_GT
static inline vuint64_t
vatomic64_await_gt(const vatomic64_t *a, vuint64_t v)
{
    return vatomic64_await_gt_rlx(a, v);
}
    #define VATOMIC64_AWAIT_GE
static inline vuint64_t
vatomic64_await_ge(const vatomic64_t *a, vuint64_t v)
{
    return vatomic64_await_ge_rlx(a, v);
}
    #define VATOMIC64_AWAIT_EQ_ACQ
static inline vuint64_t
vatomic64_await_eq_acq(const vatomic64_t *a, vuint64_t v)
{
    return vatomic64_await_eq_rlx(a, v);
}
    #define VATOMIC64_AWAIT_NEQ_ACQ
static inline vuint64_t
vatomic64_await_neq_acq(const vatomic64_t *a, vuint64_t v)
{
    return vatomic64_await_neq_rlx(a, v);
}
    #define VATOMIC64_AWAIT_LT_ACQ
static inline vuint64_t
vatomic64_await_lt_acq(const vatomic64_t *a, vuint64_t v)
{
    return vatomic64_await_lt_rlx(a, v);
}
    #define VATOMIC64_AWAIT_LE_ACQ
static inline vuint64_t
vatomic64_await_le_acq(const vatomic64_t *a, vuint64_t v)
{
    return vatomic64_await_le_rlx(a, v);
}
    #define VATOMIC64_AWAIT_GT_ACQ
static inline vuint64_t
vatomic64_await_gt_acq(const vatomic64_t *a, vuint64_t v)
{
    return vatomic64_await_gt_rlx(a, v);
}
    #define VATOMIC64_AWAIT_GE_ACQ
static inline vuint64_t
vatomic64_await_ge_acq(const vatomic64_t *a, vuint64_t v)
{
    return vatomic64_await_ge_rlx(a, v);
}

    #define VATOMIC64_AWAIT_LE_ADD
static inline vuint64_t
vatomic64_await_le_add(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_le_add_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_LE_ADD_ACQ
static inline vuint64_t
vatomic64_await_le_add_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_le_add_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_LE_ADD_REL
static inline vuint64_t
vatomic64_await_le_add_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_le_add_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_LE_SUB
static inline vuint64_t
vatomic64_await_le_sub(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_le_sub_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_LE_SUB_ACQ
static inline vuint64_t
vatomic64_await_le_sub_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_le_sub_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_LE_SUB_REL
static inline vuint64_t
vatomic64_await_le_sub_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_le_sub_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_LE_SET
static inline vuint64_t
vatomic64_await_le_set(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_le_set_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_LE_SET_ACQ
static inline vuint64_t
vatomic64_await_le_set_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_le_set_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_LE_SET_REL
static inline vuint64_t
vatomic64_await_le_set_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_le_set_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_LT_ADD
static inline vuint64_t
vatomic64_await_lt_add(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_lt_add_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_LT_ADD_ACQ
static inline vuint64_t
vatomic64_await_lt_add_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_lt_add_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_LT_ADD_REL
static inline vuint64_t
vatomic64_await_lt_add_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_lt_add_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_LT_SUB
static inline vuint64_t
vatomic64_await_lt_sub(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_lt_sub_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_LT_SUB_ACQ
static inline vuint64_t
vatomic64_await_lt_sub_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_lt_sub_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_LT_SUB_REL
static inline vuint64_t
vatomic64_await_lt_sub_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_lt_sub_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_LT_SET
static inline vuint64_t
vatomic64_await_lt_set(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_lt_set_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_LT_SET_ACQ
static inline vuint64_t
vatomic64_await_lt_set_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_lt_set_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_LT_SET_REL
static inline vuint64_t
vatomic64_await_lt_set_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_lt_set_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_GE_ADD
static inline vuint64_t
vatomic64_await_ge_add(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_ge_add_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_GE_ADD_ACQ
static inline vuint64_t
vatomic64_await_ge_add_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_ge_add_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_GE_ADD_REL
static inline vuint64_t
vatomic64_await_ge_add_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_ge_add_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_GE_SUB
static inline vuint64_t
vatomic64_await_ge_sub(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_ge_sub_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_GE_SUB_ACQ
static inline vuint64_t
vatomic64_await_ge_sub_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_ge_sub_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_GE_SUB_REL
static inline vuint64_t
vatomic64_await_ge_sub_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_ge_sub_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_GE_SET
static inline vuint64_t
vatomic64_await_ge_set(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_ge_set_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_GE_SET_ACQ
static inline vuint64_t
vatomic64_await_ge_set_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_ge_set_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_GE_SET_REL
static inline vuint64_t
vatomic64_await_ge_set_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_ge_set_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_GT_ADD
static inline vuint64_t
vatomic64_await_gt_add(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_gt_add_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_GT_ADD_ACQ
static inline vuint64_t
vatomic64_await_gt_add_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_gt_add_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_GT_ADD_REL
static inline vuint64_t
vatomic64_await_gt_add_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_gt_add_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_GT_SUB
static inline vuint64_t
vatomic64_await_gt_sub(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_gt_sub_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_GT_SUB_ACQ
static inline vuint64_t
vatomic64_await_gt_sub_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_gt_sub_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_GT_SUB_REL
static inline vuint64_t
vatomic64_await_gt_sub_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_gt_sub_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_GT_SET
static inline vuint64_t
vatomic64_await_gt_set(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_gt_set_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_GT_SET_ACQ
static inline vuint64_t
vatomic64_await_gt_set_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_gt_set_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_GT_SET_REL
static inline vuint64_t
vatomic64_await_gt_set_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_gt_set_rlx(a, c, v);
}

    #define VATOMIC64_AWAIT_NEQ_ADD
static inline vuint64_t
vatomic64_await_neq_add(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_neq_add_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_NEQ_ADD_ACQ
static inline vuint64_t
vatomic64_await_neq_add_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_neq_add_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_NEQ_ADD_REL
static inline vuint64_t
vatomic64_await_neq_add_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_neq_add_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_NEQ_SUB
static inline vuint64_t
vatomic64_await_neq_sub(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_neq_sub_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_NEQ_SUB_ACQ
static inline vuint64_t
vatomic64_await_neq_sub_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_neq_sub_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_NEQ_SUB_REL
static inline vuint64_t
vatomic64_await_neq_sub_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_neq_sub_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_NEQ_SET
static inline vuint64_t
vatomic64_await_neq_set(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_neq_set_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_NEQ_SET_ACQ
static inline vuint64_t
vatomic64_await_neq_set_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_neq_set_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_NEQ_SET_REL
static inline vuint64_t
vatomic64_await_neq_set_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_neq_set_rlx(a, c, v);
}

    #define VATOMIC64_AWAIT_EQ_ADD
static inline vuint64_t
vatomic64_await_eq_add(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_eq_add_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_EQ_ADD_ACQ
static inline vuint64_t
vatomic64_await_eq_add_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_eq_add_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_EQ_ADD_REL
static inline vuint64_t
vatomic64_await_eq_add_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_eq_add_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_EQ_SUB
static inline vuint64_t
vatomic64_await_eq_sub(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_eq_sub_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_EQ_SUB_ACQ
static inline vuint64_t
vatomic64_await_eq_sub_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_eq_sub_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_EQ_SUB_REL
static inline vuint64_t
vatomic64_await_eq_sub_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_eq_sub_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_EQ_SET
static inline vuint64_t
vatomic64_await_eq_set(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_eq_set_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_EQ_SET_ACQ
static inline vuint64_t
vatomic64_await_eq_set_acq(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_eq_set_rlx(a, c, v);
}
    #define VATOMIC64_AWAIT_EQ_SET_REL
static inline vuint64_t
vatomic64_await_eq_set_rel(vatomic64_t *a, vuint64_t c, vuint64_t v)
{
    return vatomic64_await_eq_set_rlx(a, c, v);
}

#endif
#endif
