/*
 * Copyright (C) Huawei Technologies Co., Ltd. 2023-2025. All rights reserved.
 * SPDX-License-Identifier: MIT
 */

#ifndef VATOMIC_ARM64_LLSC_H
#define VATOMIC_ARM64_LLSC_H
_tmpl_begin(=);
AUTOGEN
_tmpl_end;

#define _tmpl_mute
#include <vsync/atomic/tmplr.h>
#define _tmpl_unmute

/*******************************************************************************
 * options
 ******************************************************************************/
#if !defined(VATOMIC_ARM64_LLSC)
    #error "Don't include this file directly, include <vsync/atomic.h> instead"
#endif

_tmpl_dl; // -------------------------------------------------------------------
_tmpl_dl; // memory order mappings
_tmpl_dl; // -------------------------------------------------------------------
_tmpl_dl; // note that read/load operations don't support release, and both sc
_tmpl_dl; // and acq use acquire(a)
_tmpl_map(<READ_seq>, a);
_tmpl_map(<READ_acq>, a);
_tmpl_map(<READ_rel>, );
_tmpl_map(<READ_rlx>, );
_tmpl_dl; // note that write/store operations don't support release, and both sc
_tmpl_dl; // and rel use release (l)
_tmpl_map(<READ_seq>, a);
_tmpl_map(<WRITE_seq>, l);
_tmpl_map(<WRITE_rel>, l);
_tmpl_map(<WRITE_acq>, );
_tmpl_map(<WRITE_rlx>, );
_tmpl_dl; // read modify write that return old value support acquire
_tmpl_map(<RMW_seq>, al);
_tmpl_map(<RMW_rel>, l);
_tmpl_map(<RMW_acq>, a);
_tmpl_map(<RMW_rlx>, );
_tmpl_dl; // read modify write that store and does not return old value does not
_tmpl_dl; // support acquire
_tmpl_map(<RMW_ST_seq>, l);
_tmpl_map(<RMW_ST_rel>, l);
_tmpl_map(<RMW_ST_rlx>, );
_tmpl_dl; // -------------------------------------------------------------------
_tmpl_dl; // instruction mapping when LSE is not available
_tmpl_dl; // -------------------------------------------------------------------
_tmpl_map(<and_INST>, and);
_tmpl_map(<or_INST>, orr);
_tmpl_map(<xor_INST>, eor);
_tmpl_map(<add_INST>, add);
_tmpl_map(<sub_INST>, sub);
_tmpl_dl; // -------------------------------------------------------------------
_tmpl_dl; // instruction register mappings
_tmpl_dl; // -------------------------------------------------------------------
_tmpl_map(<u32_REG>, w);
_tmpl_map(<u64_REG>, x);
_tmpl_map(<ptr_REG>, x);
_tmpl_dl; // -------------------------------------------------------------------
_tmpl_dl; // vatomic*_xchg*
_tmpl_dl; // -------------------------------------------------------------------
_tmpl_begin(TY = [[u32; u64; ptr]], MO = [[seq; acq; rel; rlx]]);
/******************************************************************************
 * __vatomic_xchg_MS
 ******************************************************************************/
#ifndef _tmpl_upcase(__vatomic_xchg_MS)
#define _tmpl_upcase(__vatomic_xchg_MS)
static inline TT
__vatomic_xchg_MS(AA *a, TT v)
{
    TT oldv;
    vuint32_t tmp;
    __asm__ volatile(
        "prfm pstl1strm, %[a]\n"
        "1:\n"
        "ld<READ_MO>xr %<TY_REG>[oldv], %[a]\n"
        "st<WRITE_MO>xr  %w[tmp], %<TY_REG>[newv], %[a]\n"
        "cbnz %w[tmp], 1b\n"
        : [oldv] "=&r"(oldv), [tmp] "=&r"(tmp)
        : [newv] "r"(v), [a] "Q"(a->_v)
        : "memory");
    return oldv;
}
#endif
_tmpl_end;
_tmpl_dl; // -------------------------------------------------------------------
_tmpl_dl; // vatomic*_cmpxchg*
_tmpl_dl; // -------------------------------------------------------------------
_tmpl_begin(TY = [[u32; u64; ptr]], MO = [[seq; acq; rel; rlx]]);
/******************************************************************************
 * __vatomic_cmpxchg_MS
 ******************************************************************************/
#ifndef _tmpl_upcase(__vatomic_cmpxchg_MS)
#define _tmpl_upcase(__vatomic_cmpxchg_MS)
static inline TT
__vatomic_cmpxchg_MS(AA *a, TT e, TT v)
{
    TT oldv;
    vuint32_t tmp;
    __asm__ volatile(
        "prfm pstl1strm, %[a]\n"
        "1:\n"
        "ld<READ_MO>xr %<TY_REG>[oldv], %[a]\n"
        "cmp %<TY_REG>[oldv], %<TY_REG>[exp]\n"
        "b.ne 2f\n"
        "st<WRITE_MO>xr  %w[tmp], %<TY_REG>[newv], %[a]\n"
        "cbnz %w[tmp], 1b\n"
        "2:\n"
        : [oldv] "=&r"(oldv), [tmp] "=&r"(tmp)
        : [newv] "r"(v), [exp] "r"(e), [a] "Q"(a->_v)
        : "memory", "cc");
    return oldv;
}
#endif
_tmpl_end;
_tmpl_dl; // -------------------------------------------------------------------
_tmpl_dl; // vatomic*_get_max*
_tmpl_dl; // -------------------------------------------------------------------
_tmpl_begin(TY = [[u32; u64]], MO = [[seq; acq; rel; rlx]]);
/******************************************************************************
 * __vatomic_get_max_MS
 ******************************************************************************/
#ifndef _tmpl_upcase(__vatomic_get_max_MS)
#define _tmpl_upcase(__vatomic_get_max_MS)
static inline TT
__vatomic_get_max_MS(AA *a, TT v)
{
    vuint32_t tmp;
    TT oldv;
    __asm__ volatile(
        "prfm pstl1strm, %[a]\n"
        "1:\n"
        "ld<READ_MO>xr %<TY_REG>[oldv], %[a]\n"
        "cmp %<TY_REG>[oldv], %<TY_REG>[v]\n"
        "b.hs 2f\n"
        "st<WRITE_MO>xr %w[tmp], %<TY_REG>[v], %[a]\n"
        "cbnz %w[tmp], 1b\n"
        "2:\n"
        : [oldv] "=&r"(oldv), [tmp] "=&r"(tmp), [v] "+&r"(v)
        : [a] "Q"(a->_v)
        : "memory", "cc");

    return oldv;
}
#endif
_tmpl_end;
_tmpl_dl; // -------------------------------------------------------------------
_tmpl_dl; // vatomic*_max*
_tmpl_dl; // -------------------------------------------------------------------
_tmpl_begin(TY = [[u32; u64]], MO = [[seq; rel; rlx]]);
/******************************************************************************
 * __vatomic_max_MS
 ******************************************************************************/
#ifndef _tmpl_upcase(__vatomic_max_MS)
#define _tmpl_upcase(__vatomic_max_MS)
static inline void
__vatomic_max_MS(AA *a, TT v)
{
    TT oldv;
    vuint32_t tmp;
    __asm__ volatile(
        "prfm pstl1strm, %[a]\n"
        "1:\n"
        "ld<READ_MO>xr %<TY_REG>[oldv], %[a]\n"
        "cmp %<TY_REG>[oldv], %<TY_REG>[v]\n"
        "b.hs 2f\n"
        "st<WRITE_MO>xr  %w[tmp], %<TY_REG>[v], %[a]\n"
        "cbnz %w[tmp], 1b\n"
        "2:\n"
        : [oldv] "=&r"(oldv), [tmp] "=&r"(tmp), [v] "+&r"(v)
        : [a] "Q"(a->_v)
        : "memory", "cc");
}
#endif
_tmpl_end;
_tmpl_dl; // -------------------------------------------------------------------
_tmpl_dl; // vatomic*_get_(and/or/xor/add/sub)*
_tmpl_dl; // -------------------------------------------------------------------
_tmpl_begin(TY = [[u32; u64]], MO = [[seq; acq; rel; rlx]],
            OP = [[and; or ; xor; add; sub]]);
/******************************************************************************
 * __vatomic_get_OP_MS
 ******************************************************************************/
#ifndef _tmpl_upcase(__vatomic_get_OP_MS)
#define _tmpl_upcase(__vatomic_get_OP_MS)
static inline TT
__vatomic_get_OP_MS(AA *a, TT v)
{
    TT oldv;
    vuint32_t tmp;
    TT newv;
    __asm__ volatile(
        "prfm pstl1strm, %[a]\n"
        "1:\n"
        "ld<READ_MO>xr %<TY_REG>[oldv], %[a]\n"
        "<OP_INST> %<TY_REG>[newv], %<TY_REG>[oldv], %<TY_REG>[v]\n"
        "st<WRITE_MO>xr %w[tmp], %<TY_REG>[newv], %[a]\n"
        "cbnz %w[tmp], 1b\n"
        : [oldv] "=&r"(oldv), [newv] "=&r"(newv), [tmp] "=&r"(tmp), [v] "+&r"(v)
        : [a] "Q"(a->_v)
        : "memory", "cc");

    return oldv;
}
#endif
_tmpl_end;
_tmpl_dl; // -------------------------------------------------------------------
_tmpl_dl; // vatomic*_(and/or/xor/add/sub)*
_tmpl_dl; // -------------------------------------------------------------------
_tmpl_begin(TY = [[u32; u64]], MO = [[seq; rel; rlx]],
            OP = [[and; or ; xor; add; sub]]);
/******************************************************************************
 * __vatomic_OP_MS
 ******************************************************************************/
#ifndef _tmpl_upcase(__vatomic_OP_MS)
#define _tmpl_upcase(__vatomic_OP_MS)
static inline void
__vatomic_OP_MS(AA *a, TT v)
{
    TT oldv;
    TT newv;
    vuint32_t tmp;
    __asm__ volatile(
        "prfm pstl1strm, %[a]\n"
        "1:\n"
        "ld<READ_MO>xr %<TY_REG>[oldv], %[a]\n"
        "<OP_INST> %<TY_REG>[newv], %<TY_REG>[oldv], %<TY_REG>[v]\n"
        "st<WRITE_MO>xr %w[tmp], %<TY_REG>[newv], %[a]\n"
        "cbnz %w[tmp], 1b\n"
        : [oldv] "=&r"(oldv), [newv] "=&r"(newv), [tmp] "=&r"(tmp), [v] "+&r"(v)
        : [a] "Q"(a->_v)
        : "memory", "cc");
}
#endif
_tmpl_end;
#endif
